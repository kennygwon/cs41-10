\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{clrscode3e}
\usepackage{url}
\usepackage{amssymb} %lots of math symbols
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{graphicx,float}

\DeclareMathOperator{\ptr}{\leq_P}
\DeclareMathOperator{\deq}{:=}

\newtheorem{lemma}{Lemma}

\newcommand{\yes}{\textsc{yes}\xspace}
\newcommand{\no}{\textsc{no}\xspace}
\newcommand{\true}{\textsc{true}\xspace}
\newcommand{\false}{\textsc{false}\xspace}

\newcommand{\prob}[1]{\textsc{#1}\xspace}
\newcommand{\vco}{\prob{VC-Opt}}
\newcommand{\vc}{\prob{Vertex-Cover}}
\newcommand{\vcopt}{\prob{Min-Vertex-Cover}}
\newcommand{\is}{\prob{Independent-Set}}
\newcommand{\isopt}{\prob{Max-Independent-Set}}
\newcommand{\sat}{\prob{Sat}}
\newcommand{\tsat}{\prob{3-Sat}}
\newcommand{\ksat}{\prob{k-Sat}}
\newcommand{\factor}{\prob{Factoring}}
\newcommand{\nfactor}{\prob{Not-Factoring}}
\newcommand{\primes}{\prob{Primes}}
\newcommand{\tdm}{\prob{3d-Matching}}
\newcommand{\tcolor}{\prob{Three-Coloring}}
\newcommand{\wedding}{\prob{Wedding-Planner}}
\newcommand{\clique}{\prob{Clique}}
\newcommand{\cliqueopt}{\prob{Max-Clique}}
\newcommand{\tsp}{\prob{TSP}}
\newcommand{\metrictsp}{\prob{Metric-TSP}}

\newcommand{\np}{\textsc{NP}\xspace}
\newcommand{\npc}{\textsc{NPC}\xspace}
\newcommand{\npcomplete}{\textsc{NP-Complete}\xspace}
\newcommand{\p}{\textsc{P}\xspace}

\newcommand{\titlebox}[6]{
    \begin{center}
        \framebox{
            \vbox{
            \hbox to 5.78in { {\bf #1} \hfill #2}
            \vspace{4mm}
            \hbox to 5.78in { {\Large \hfill #3 \hfill} }
            \vspace{2mm}
            \hbox to 5.78in { {\it Due: #4 \hfill #5} }
        }
    }
    \end{center}
}

\begin{document}
\titlebox{CPSC 41: Algorithms} % course number and title
{Kastan Day, Kenny Gwong} % student(s) name(s) --- this means you (pl)!
{Homework 10} % assignment number
{11:59pm December 6, 2017} % due date
{Professor Bryce Wiedenbeck} % which prof's section

\vspace{0.2in} This homework is due {\bf 11:59pm Wednesday, December 6.}
%
Submit this homework as a {\tt *.tex} file using \textbf{github}.
%
For this homework, you will work with a partner.  It's ok to discuss
approaches with others at a high level, but most of your discussions
should just be with your partner.  The only exception to this rule
is work you've done with a lab partner \emph{while in lab}.  In this
case, note who you've worked with and what parts were solved during
lab.  If there are questions about academic integrity, please visit
the section on Academic Integrity on the course website.


\begin{enumerate}
  \setcounter{enumi}{-1}
\item Before final submission, make sure to fill out the README file.

  \item {\bf (K\&T 11.3)}
    Suppose you are given a set of positive integers $A=\{a_1, a_2,
    \ldots, a_n\}$ and a positive integer $B$.
    A subset $S \subseteq A$ is called \emph{feasible} if the sum of
    the numbers in $S$ does not exceed $B$:
    $$ \sum_{a_i \in S} a_i \leq B.$$
    The sum of the numbers in $S$ will be called the \emph{total sum}
    of $S$.

    You would like to select a feasible subset $S$ of $A$ whose total
    sum is as large as possible.
    
    For example, if $A=\{8,2,4\}$ and $B=11$ then the optimal solution
    is the subset $S=\{8,2\}$.

    \begin{enumerate}
      \item
      Here is an algorithm for this problem.
      \begin{codebox}
        \Procname{\proc{notQuiteRight}($A=\{a_1,\ldots, a_n\}, B$)}
        \li initialize $S = \emptyset$
        \li define $T=0$
        \li \For $i=1$ to $n$: \Do
        \li \If $T + a_i \leq B$ \Then
        \li $S \gets S \cup \{a_i\}$
        \li $T \gets T + a_i$
        \End % if
        \End % for
        \li return $S$
      \end{codebox}
      Give an input for which the total sum of the set $S$ returned by
      this algorithm is less than half the total sum of some other
      feasible subset of $A$. (You don't necessarily have to find the
      optimal subset, just \emph{some} feasible subset.)

    \item Give a polynomial-time approximation algorithm for this
      problem with the following guarantee:
      It returns a feasible set $S \subseteq A$ whose total sum is at
      least half as large as the maximum total sum of any feasible set
      $S' \subseteq A$.
      Your algorithm should run asymptotically faster than $O(n^2)$.
    \end{enumerate}
    
    \textbf{Our Response}
    \begin{enumerate}
        \item A = \{2, 3, 9\} B = 11
    It would pick 2 and 3, but then be unable to pick 9 and not returning the optimal set of 11 from 2 and 9. The sum of the set returned by the algorithm is 5 which is less than one half of 11.
    
        \item Consider the following polynomial time algorithm:
        
        \begin{codebox}
            \Procname{MaxFeasibleApprox($A=\{a_1,\ldots, a_n\}, B$)}
            \li $A \leftarrow$ sort $A$ from largest to smallest using MergeSort
            \li initialize $S = \emptyset$
            \li define $T=0$
            \li \For $i=1$ to $n$: \Do
            \li \If $T + a_i \leq B$ \Then
            \li $S \gets S \cup \{a_i\}$
            \li $T \gets T + a_i$
            \End % if
            \End % for
            \li return $S$
        \end{codebox}
        This algorithm is runs in $O(n logn)$ time because it uses mergesort on line (1) which runs in $O(n logn)$.  Then it runs \proc{NotQuiteRight} which runs in $O(n)$.  Therefore the total runtime is $O(n logn) + O(n)$ or just $O(n logn)$.  This is asymptotically faster than $O(n^2)$.
        
        Now we show that \proc{MaxFeasibleApprox} returns a feasible set whose total sum is at least half as large as the maximum total sum of any feasible set.
        Let $S$ be the total sum of the feasible set returned by \proc{MaxFeasibleApprox} and $S'$ be the maximum total sum of any feasible set.  Let $B$ be the integer that the sum of a feasible set cannot exceed.  Let $L$ be the largest value in $S$ and its position in $A$ denoted by $a_j$.  We want to show that $S \geq \frac{1}{2}S'$.  We know that $S' \leq B$ because if $S' > B$, $S'$ would not be a feasible subset. We also know that if $L > \frac{1}{2}B$ than $S \geq \frac{1}{2}S'$ because $L$ is in $S$.  After our algorithm selects $L$ it considers adding the rest of the elements in $A$.  If $\sum_{j+1}^{n}a_i \geq \frac{1}{2}B$, then we know that $S$ will be at least $\frac{1}{2}B$ because the items in $A$ are decreasing therefore there can't be an item in $S'$ that is greater or equal to $B-L$.  If $\sum_{j+1}^{n}a_i < \frac{1}{2}B$ then we know that $S$ = $S'$ because $S$ chooses all the values that are less than $B$ and therefore $S \geq \frac{1}{2}S'$. 
    \end{enumerate}

\item {\bf Reductions.}
  Recall that many problems have a decision version and an
  optimization version, so for example we can consider the problems
  \begin{itemize}
    \item $\is(G,k)$ returns $\yes$ iff there is an independent set in $G$ of
      size $\geq k$,
    \item $\isopt(G)$ returns the size of the largest independent set in
      $G$, 
    \item $\vc(G,k)$ returns $\yes$ iff there is a vertex cover of $G$ of size
      at most $k$, 
    \item $\vcopt(G)$ returns the size of the smallest vertex
      cover of $G$,
    \item $\clique(G,k)$ returns $\yes$ iff there is a clique in $G$
      of size $k$, and
    \item $\cliqueopt(G)$ returns the size of the largest
      clique\footnote{A {\bf clique} is a set of nodes $C \subseteq V$
        such that every two vertices $u, v \in C$ are connected by an
        edge: $\{u,v\} \in E$.} in $G$.
  \end{itemize}

  We know that all \npcomplete problems reduce to each other. It would
  be nice if this meant that an approximation algorithm for one
  \npcomplete problem can be adapted easily into an equally good
  approximation algorithm for any other \npcomplete problem.
  
  \begin{enumerate}
  \item We proved that \vc $\leq_p$ \is by giving an
    algorithm for \vc that used an algorithm for \is as a black box.

    Suppose we wanted to solve the optimization versions of these
    problems, \vcopt and \isopt.
    Can we do the same thing with approximation algorithms?

    That is, suppose we have a black box $2$-approximation algorithm
    for \isopt. 
    Design an approximation algorithm for \vcopt
    using (as a black box) a $2$-approximation algorithm for \isopt.
    (Your algorithm should be based on the reduction.)

    What kind of approximation guarantee can you give?
    Either prove an approximation ratio of your algorithm, or explain
    why this ratio is hard to calculate.

  \item Assume we have an $k$-approximation algorithm for \cliqueopt
    where $k$ is a constant.  Can we use this to
    construct a decent approximation algorithm for \isopt?
    Justify your answer by designing an
    approximation algorithm for \isopt, and either proving an
    approximation ratio or explaining why this ratio is hard to
    calculate.
  \end{enumerate}
  
  \textbf{Our response:} 
  
  \begin{enumerate}
      \item First, let us define the relationship between the optimization versions of \vc (MVC) and \is (MIS). 
      Any graph $G = (V,E)$ has a max-independent-set $S$ if and only if $V \setminus S$ is a vertex-cover in $G$.  Let's call this vertex-cover $C$. Therefore, we can say that $|V| = |S| + |C|$ where $|C|$ is actually the size of not only \textit{a} vertex cover but the size of the a min-vertex-cover. 
      
      However, even though this seems as though a 2-approximation of MIS would yield a useful approximation of MVC, and under certain conditions it indeed can, we have no guarantees that it will actually be useful.  
      
      Take, for instance, that some graph $G$ has an optimal MIS of $S*$, where $|S*| \geq |V|/2$.  The size of the MIS is greater than or equal to half of the number of vertices.  In this case, the 2-approximation for MVC is only guaranteed to give a min-vertex-cover of size $|V|$, the entire set (or potentially worse, if the algorithm does not check its bounds!). This is a trivial answer and an unhelpful bound.  Therefore, a 2-approximation for max-independent-set does not offer any helpful information for determining the size, or vertices in, of the min-vertex-cover. 
      
      \item  If we assume we have a $k$-approx algorithm for \cliqueopt, where $k$ is a constant, then we can show that we can use that approximation algorithm to easily construct a  $k$-approximation algorithm for \isopt, with the same degree of approximation.
      
      To do this, all we have to do to set up our input to use \cliqueopt as a 'black box algorithm' is to 'flip the edges' - that is, remove all edges that existed in the original graph, and create an edge between each pair of nodes for which no edge existed in the original graph.  Then we can run \cliqueopt and that will return a set of nodes which forms a $k$-approx for \isopt.  
      
      This is because if we know a graph's Max-Clique, by simply 'flipping the edges,' as described above, that same set of nodes will form that graph's Max-Independent-Set, by the properties of Clique and Independent Set.  Therefore, since no transformation of the output is required, because the exact same set of nodes are used, we will have the same quality approximation for \isopt as we did for our $k$-approximation of \cliqueopt.
      
    \begin{codebox}
    \Procname{\isopt $k$-approx($G=(V,E)$)}
    \li $G' \leftarrow G$
    \li \For each edge $e \in G'$: \# remove all edges from original graph
        \Do 
        \li delete $e$ from $G'$
        \End
    \li \For each node $u \in G'$: \# construct all edges which didn't exist in the original graph
        \Do
        \li \For each node $v \in G'$:
            \Do
            \li \If edge $e = (u, v)$ $\nexists$ in $G$:
            \Do
                \li add edge $e$ to $G'$
            \End
        \End
    \End
    \li Run k-approx algorithm \cliqueopt($G'$)
    \li \Return $G'$
    \end{codebox}
      
    
  \end{enumerate}

\item {\bf Coloring.} 

  Suppose we're somehow told that a graph is
  three-colorable. Could that help us color the graph?  In this
  problem, you'll shoot for a different kind of approximation.  Give a
  polynomial time deterministic algorithm that, given any
  \emph{three-colorable} graph $G = (V,E)$, colors the graph using
  $O(\sqrt{n})$ colors.  Note that the endpoints of each edge
  \emph{must} be different colors, and you're given that it is
  \emph{possible} to color the graph using just three colors, but you
  don't know what the coloring is.

  Here are a few hints to help you along:
  \begin{enumerate}
  \item First, give a simple greedy algorithm that, given a graph $G =
    (V,E)$ such that each vertex has at most $d$ neighbors, colors $G$
    using only $d+1$ colors.
  \item Second, recall the algorithm for deciding if a graph is
    \emph{bipartite}.
  \item Third, start coloring the three-colorable graph taking the
    vertex with the most neighbors, and coloring those neighbors using
    just two colors.
  \end{enumerate}
  
  \textbf{Our response:} 
  
Consider the following polynomial time algorithm where $d(v)$ denotes the largest degree of $v$ and $n$ = $|V|$:
      
    \begin{codebox}
        \Procname{ThreeColorApprox($G=(V,E)$)}
        \li $G' \leftarrow G$
        \li \While $d(v)$ in $G' \geq \sqrt{n}$ \Do
        \li color the neighbors of $v$ in $G$ using DFS and with only two colors
        \li delete the neighbors of $v$ and corresponding edges in $G'$ \End
        \li list $c$ of $\sqrt{n}$ colors
        \li \For each node $w$ in $G'$ \Do
        \li $d \leftarrow c$
        \li \If $w$ not colored \Do
        \li remove the colors of the neighbors of $w$ from $d$
        \li color $w$ in $G$ and $G'$ a color from $d$ \End \End \End
        \li \Return $G$
    \end{codebox}
    
    The intuition behind our algorithm is we know that the neighbors of each vertex in a three-colorable graph form a bipartite graph because if the neighbors of a vertex form an odd cycle then the graph is not three-colorable.  Therefore we can take any vertex with $\sqrt{n}$ or more neighbors and use two colors to color all of its neighbors since the neighbors form a bipartite graph.  This will run at most $\sqrt{n}$ times because each time we are coloring $\sqrt{n}$ vertices and $\frac{n}{\sqrt{n}} = \sqrt{n}$.  This portion of the algorithm runs on lines (1)-(4) and will use $2\sqrt{n}$ colors since it uses two colors for each iteration of the loop and runs a maximum of $\sqrt{n} $ iterations.  Once we do this for vertices with more than $\sqrt{n}$ neighbors, the vertex with the greatest number of vertices will have less than $\sqrt{n}$ neighbors.  We can than use $\sqrt{n}$ colors to color the remaining vertices since we can use a greedy algorithm that given a maximum of $d$ neighbors, colors the vertices with $d+1$ neighbors. This portion of the algorithm runs on lines (5)-(10).  Therefore since the first portion of the algorithm used $2 \sqrt{n}$ colors and the second portion used $\sqrt{n}$ colors, the maximum number of colors used is $3\sqrt{n}$ colors which is $O(\sqrt{n})$. 
    
    Now we will show the runtime of the algorithm is polynomial.  Copying a graph in line (1) runs in constant time.  The while loop on line(2) runs $\sqrt{n}$ iterations.  Finding the node with the largest degree in $G'$ is $O(n^2)$ if we use an adjacency matrix because finding the degree of each node takes $O(n)$ time and there are $n$ nodes.  Then sorting the degrees to find the largest takes $O(nlogn)$ time which is negligible because it is less than $O(n^2)$.  Coloring the neighbors of a node on line (3) using DFS takes $O(n)$ time because we iterate through each node once and each node we visit has no more than one uncolored neighbor.  Deleting the neighbors of a node and its corresponding edges in line (4) takes $O(n)$ time.  So each iteration of the while loop on lines (2)-(6) does $O(n^2)+O(n)+O(n)$ work and runs $\sqrt{n}$ iterations.  Thus the amount of work done on lines (2)-(6) $O(n^{3/2})$ which runs in polynomial time.  Creating a list on line (5) takes constant time.  The for loop on line (6) runs a maximum of $n$ iterations.  Copying a list on line (7) runs in constant time.  Removing the colors of the neighbors from a list takes $O(\sqrt{n})$ work because there a maximum of $\sqrt{n}$ neighbors.  Coloring a node in two graphs on line (10) takes constant time.  Therefore the amount of work in the if condition on lines (8)-(10) is $O(\sqrt{n})$.  The total amount of work done in the for loop on lines (6)-(10) is $O(\sqrt{n})$ and the for loop runs $n$ iterations so the total amount of work done on lines (6)-(10) is $n^{3/2}$ which is polynomial time.  And finally the return statement on line (11) takes $O(1)$ which is less than polynomial time.  So the total runtime of the algorithm is $O(1)+O(n^{3/2})+O(n^{3/2})+O(1))$ or $O(n^{3/2})$ which is polynomial time.  Thus our algorithm runs in polynomial time.

  \item {\bf Extra credit.}
    Show that the triangle inequality assumption is necessary to
    achieve the $2$-approximation for \metrictsp.  Specifically, show
    that if there is a polynomial-time approximation for general \tsp
    with $\rho(n)=O(1)$, then $\p = \np$.
\end{enumerate}

\end{document}
